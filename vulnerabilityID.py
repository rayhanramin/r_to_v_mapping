import requests
from bs4 import BeautifulSoup
import csv
from prettytable import PrettyTable


def grab_bug_id(url):
    response = requests.get(url)
    html_code = response.content

    # Use BeautifulSoup to parse the HTML code
    soup = BeautifulSoup(html_code, 'html.parser')

    # Find the table
    table = soup.find('table')

    for i in range(1, 2754):
        # Find the second row in the table
        rows = table.find_all('tr')
        current_row = rows[i]

        # Find the second column in the second row
        columns = current_row.find_all('td')
        second_column = columns[1]
        third_column = columns[2]

        # Extract the text from the element
        bugID = second_column.get_text()
        bug_url = third_column.get_text()

        original_url = third_column.get_text()

        vulnID = search_webpage_for_vuln(bug_url)
        if vulnID != None:
            bug_vuln_table.add_row([bugID, original_url, vulnID])


def create_url(bug):
    url = 'https://bugzilla.mozilla.org/show_bug.cgi?id=' + bug
    return url


def search_webpage_for_vuln(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    results = []

    for tag in soup.find_all(class_='edit-hide'):
        if tag.string:
            string = tag.string.strip()
            if string.startswith('(') and string.endswith(')'):
                results.append(string)
                vuln_id = str(results)[3:-3]
                return vuln_id


bug_vuln_table = PrettyTable(["Bug ID", "Bug ID Link", "Vulnerability ID"])

# Give Github URL
url = 'https://github.com/rayhanramin/monsterjobscrapper/blob/master/bugfixing_commits1.csv'
# Get bug id
bugID = grab_bug_id(url)

with open("bug_vuln_table.csv", "w", newline='') as csvfile:
    csvfile.write(bug_vuln_table.get_csv_string())
